{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer-based Time Series Classification\n",
   "id": "4adaccdb21b4294b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:21:16.013521Z",
     "start_time": "2025-07-18T09:21:06.025339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from parser import load_saved_dataframe\n",
    "from helper import *\n"
   ],
   "id": "5b4e05eb56674aab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset\n",
   "id": "13d318839b100edb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:21:16.024522Z",
     "start_time": "2025-07-18T09:21:16.017376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    row['timeseries_V1'],\n",
    "                    row['timeseries_V2'],\n",
    "                    row['timeseries_V3'],\n",
    "                    row['timeseries_V4'],\n",
    "                    row['timeseries_V5'],\n",
    "                    row['timeseries_V6'],\n",
    "                ]\n",
    "                for _, row in df.iterrows()\n",
    "            ],\n",
    "            dtype=torch.float32\n",
    "        )  # shape: (samples, 6, series_length)\n",
    "        self.y = torch.tensor(\n",
    "            [0 if row['group'] == 'control' else 1 for _, row in df.iterrows()],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ],
   "id": "5fd7e53021a11a74",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer Model\n",
   "id": "da338c04288f2986"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:21:16.138073Z",
     "start_time": "2025-07-18T09:21:16.132895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=6, num_classes=2, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_channels, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, seq_len) -> (batch, seq_len, channels)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.input_proj(x)  # (batch, seq_len, d_model)\n",
    "        x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
    "        x = x.permute(0, 2, 1)  # (batch, d_model, seq_len)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (batch, d_model)\n",
    "        x = self.fc(x)  # (batch, num_classes)\n",
    "        return x\n"
   ],
   "id": "9804275c7bf0510d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions\n",
   "id": "bfb0a4dc4c7b3a0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:21:16.171837Z",
     "start_time": "2025-07-18T09:21:16.161020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_class_weights(y):\n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    class_weights = {}\n",
    "    for class_id, count in class_counts.items():\n",
    "        class_weights[class_id] = total_samples / (len(class_counts) * count)\n",
    "    return class_weights\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f2 = fbeta_score(all_targets, all_predictions, beta=2, average='weighted')\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    return avg_loss, f2, precision, recall\n",
    "\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader,\n",
    "                                    criterion, optimizer, scheduler,\n",
    "                                    device, num_epochs=10,\n",
    "                                    patience=10, min_delta=0.001):\n",
    "    \"\"\"Train model with early stopping based on validation F2 score\"\"\"\n",
    "\n",
    "    # History tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f2_scores = []\n",
    "    val_f2_scores = []\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_f2 = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Training for up to {num_epochs} epochs with early stopping...\")\n",
    "    print(f\"Early stopping patience: {patience} epochs\")\n",
    "    print(f\"Minimum improvement delta: {min_delta}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        # Compute train F2 for this epoch\n",
    "        train_f2 = fbeta_score(train_targets, train_preds, beta=2, average='weighted')\n",
    "        train_f2_scores.append(train_f2)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_f2, val_precision, val_recall = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Track metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_f2_scores.append(val_f2)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch < 10:\n",
    "            print(f\"Epoch {epoch + 1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val F2: {val_f2:.4f} | \"\n",
    "                  f\"Val Precision: {val_precision:.4f} | \"\n",
    "                  f\"Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_f2 > best_val_f2 + min_delta:\n",
    "            best_val_f2 = val_f2\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best validation F2: {val_f2:.4f} (epoch {epoch + 1})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            print(f\"Best validation F2: {best_val_f2:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model weights\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f2_scores': train_f2_scores,\n",
    "        'val_f2_scores': val_f2_scores,\n",
    "        'best_val_f2': best_val_f2,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            targets.extend(y_batch.numpy())\n",
    "    return np.array(targets), np.array(preds)\n"
   ],
   "id": "d8bd1734fcb55b7a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading and Splitting\n",
   "id": "4449f9be07207935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:21:20.212188Z",
     "start_time": "2025-07-18T09:21:16.173306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = load_saved_dataframe(\"timeseries_data.pkl\")\n",
    "series_length = len(df.iloc[0]['timeseries_V1'])\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['group']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['group']\n",
    ")\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_df)\n",
    "val_dataset = TimeSeriesDataset(val_df)\n",
    "test_dataset = TimeSeriesDataset(test_df)\n",
    "\n",
    "train_labels = [0 if row['group'] == 'control' else 1 for _, row in train_df.iterrows()]\n",
    "class_weights = calculate_class_weights(train_labels)\n",
    "weight_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "35d900ca0bdcf069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully from timeseries_data.pkl\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model, Optimizer, Training\n",
   "id": "e3e64ee4647ba1b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-18T09:21:20.301522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TimeSeriesTransformer(\n",
    "    input_channels=6,\n",
    "    num_classes=2,\n",
    "    d_model=8,\n",
    "    nhead=1,\n",
    "    num_layers=1\n",
    ")\n",
    "model.to(device)\n",
    "weight_tensor = weight_tensor.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "history = train_model_with_early_stopping(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=50,\n",
    "    patience=15,\n",
    "    min_delta=0.001\n",
    ")"
   ],
   "id": "a1d0aa1a6dbd2b98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brill\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for up to 50 epochs with early stopping...\n",
      "Early stopping patience: 15 epochs\n",
      "Minimum improvement delta: 0.001\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training History Plots\n",
   "id": "b7e3dd461decf99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_training_history(history)",
   "id": "d02ca8e91e8fdeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation\n",
   "id": "7e53951ce500c797"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_loss, val_f2, val_precision, val_recall = evaluate_model(\n",
    "    model, val_loader, criterion, device\n",
    ")\n",
    "test_loss, test_f2, test_precision, test_recall = evaluate_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation F2 Score: {val_f2:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test F2 Score: {test_f2:.4f}\")\n",
    "print(f\"Best Validation F2: {history['best_val_f2']:.4f}\")\n",
    "print(f\"Total Epochs Trained: {history['epochs_trained']}\")"
   ],
   "id": "1b69e5738b03fd39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Confusion Matrices\n",
   "id": "8db90e66f35aa1aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_targets, val_preds = get_predictions(model, val_loader, device)\n",
    "test_targets, test_preds = get_predictions(model, test_loader, device)\n",
    "\n",
    "plot_confusion_matrices(\n",
    "    [val_targets, test_targets],\n",
    "    [val_preds, test_preds],\n",
    "    [\"Validation Confusion Matrix\", \"Test Confusion Matrix\"]\n",
    ")"
   ],
   "id": "60273b1d3899d4b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Model",
   "id": "f5f9beae78548ee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_f2': history['best_val_f2'],\n",
    "    'test_f2': test_f2,\n",
    "    'class_weights': class_weights,\n",
    "    'history': history\n",
    "}, 'best_transformer_model.pth')\n",
    "print(f\"\\nModel saved as 'best_transformer_model.pth'\")"
   ],
   "id": "926e96c05c6edcd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
